{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable ml.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir friday_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch ./friday_training/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./friday_training/train.py\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import subprocess\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "import datetime\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# Fill in your Cloud Storage bucket name\n",
    "BUCKET_ID = 'friday_demo2'\n",
    "\n",
    "public_bucket = storage.Client().bucket(BUCKET_ID)\n",
    "blob = public_bucket.blob('Data/train.csv')\n",
    "blob.download_to_filename('train.csv')\n",
    "\n",
    "blob = public_bucket.blob('Data/test.csv')\n",
    "blob.download_to_filename('test.csv')\n",
    "\n",
    "#Read the data from the bucket\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "\n",
    "numeric_features = train.select_dtypes(include=[np.number])\n",
    "numeric_features.dtypes\n",
    "\n",
    "\n",
    "# categorical columns to convert\n",
    "categorical_columns = [\"Gender\", \"Age\", \"Occupation\", \"City_Category\", \"Stay_In_Current_City_Years\",\n",
    "                       \"Marital_Status\", \"Product_Category_1\"]\n",
    "\n",
    "\n",
    "# Join Train and Test Dataset so it can be cleaned all at once\n",
    "train['source']='train'\n",
    "test['source']='test'\n",
    "\n",
    "data = pd.concat([train,test], ignore_index = True, sort = False)\n",
    "\n",
    "#Get index of all columns with product_category_1 equal 19 or 20 from train and remove since not populated\n",
    "condition = data.index[(data.Product_Category_1.isin([19,20])) & (data.source == \"train\")]\n",
    "data = data.drop(condition)\n",
    "\n",
    "# define example\n",
    "#community_area = [num for num in range(78)]\n",
    "# data = array(data)\n",
    "#print(community_area)\n",
    "# one hot encode\n",
    "\n",
    "# convert categorical data to to numerical values.\n",
    "# convert data in categorical columns to numerical values\n",
    "\"\"\"encoders = {col:LabelEncoder() for col in categorical_columns}\n",
    "for col in categorical_columns:\n",
    "    data[col] = encoders[col].fit_transform(data[col])\n",
    "    #data[col] = to_categorical(data[col])\n",
    "    #print(data[col])\n",
    "\"\"\"\n",
    "data = pd.get_dummies(data, columns=categorical_columns, drop_first=False)\n",
    "#print(data)\n",
    "\n",
    "totalitem = data['User_ID'].value_counts().sort_index()\n",
    "totalpurchase = data.groupby('User_ID').sum()['Purchase']\n",
    "tot = pd.concat([totalitem, totalpurchase], axis =1, keys = ['Total_products', 'Total_purchase'])\n",
    "data = pd.merge(data, tot, left_on = 'User_ID', right_index = True)\n",
    "    \n",
    "#Divide into test and train\n",
    "train = data.loc[data['source']==\"train\"]\n",
    "test = data.loc[data['source']==\"test\"]\n",
    "\n",
    "#Drop unnecessary columns:\n",
    "test.drop(['source'],axis=1,inplace=True)\n",
    "train.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "# remove column we are trying to predict ('Purchase') from features list and also removed product category 2 and 3 due to missing values\n",
    "train_features = train.drop(['Purchase', 'Product_Category_2', 'Product_Category_3', 'Product_ID', 'User_ID'], axis=1)\n",
    "test_features = test.drop(['Purchase', 'Product_Category_2', 'Product_Category_3', 'Product_ID', 'User_ID'], axis=1)\n",
    "# create training labels list\n",
    "train_labels = train[['Purchase']]\n",
    "test_labels = test[['Purchase']]\n",
    "\n",
    "# load data into DMatrix object\n",
    "dtrain = xgb.DMatrix(train_features, train_labels)\n",
    "dtest = xgb.DMatrix(test_features)\n",
    "# train model\n",
    "bst = xgb.train({}, dtrain, 20)\n",
    "\n",
    "\n",
    "# Export the model to a file\n",
    "model = 'model.bst'\n",
    "bst.save_model('./model.bst')\n",
    "\n",
    "# Upload the model to Cloud Storage\n",
    "bucket = storage.Client().bucket(BUCKET_ID)\n",
    "blob = bucket.blob(model)\n",
    "blob.upload_from_filename(model)\n",
    "\n",
    "\n",
    "#tree based learner for onehotencode\n",
    "from sklearn.metrics import mean_squared_error\n",
    "xg_reg = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 200)\n",
    "\n",
    "xg_reg.fit(train_features,train_labels)\n",
    "\n",
    "preds = xg_reg.predict(train_features)\n",
    "rmse = np.sqrt(mean_squared_error(train_labels, preds))\n",
    "print(\"RMSE: %f\" % (rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tree based learner for onehotencode\n",
    "from sklearn.metrics import mean_squared_error\n",
    "xg_reg = xgb.XGBRegressor(colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 200)\n",
    "\n",
    "fit = xg_reg.fit(train_features,train_labels)\n",
    "\n",
    "preds = xg_reg.predict(train_features)\n",
    "rmse = np.sqrt(mean_squared_error(train_labels, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "n_estimators = range(50, 400, 50)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(xg_reg, param_grid, scoring=\"neg_mean_squared_error\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(train_features, train_labels)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k fold cross validation\n",
    "params = {\"objective\":\"reg:squarederror\",'colsample_bytree': 0.3,'learning_rate': 0.1,\n",
    "                'max_depth': 5, 'alpha': 10}\n",
    "\n",
    "cv_results = xgb.cv(dtrain=dtrain, params=params, nfold=3,\n",
    "                    num_boost_round=250,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.tail(50)\n",
    "#print((cv_results[\"test-rmse-mean\"]).tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "xgb.plot_importance(xg_reg)\n",
    "plt.rcParams['figure.figsize'] = [50, 50]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor()\n",
    "xgb.fit(train_features, train_labels)\n",
    "imp = pd.DataFrame(xgb.feature_importances_ ,columns = ['Importance'],index = train_features.columns)\n",
    "imp = imp.sort_values(['Importance'], ascending = False)\n",
    "\n",
    "print(imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, GridSearchCV\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = XGBRegressor( learning_rate =0.07, n_estimators=1000, max_depth=5,\n",
    " min_child_weight=1.5, gamma=0.03, subsample=0.95, colsample_bytree=0.4,\n",
    " nthread=4, scale_pos_weight=1, seed=27, reg_alpah=0.75, reg_lambda=0.45), \n",
    " param_grid = param_test1, scoring='neg_mean_squared_error',n_jobs=4,iid=False, cv=5)\n",
    "gsearch1.fit(train_features,train_labels)\n",
    "gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Define a timestamped job name\n",
    "JOB_NAME = \"friday_training_{}\".format(int(time.time()))\n",
    "BUCKET_NAME = 'friday_demo2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Submit the training job:\n",
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --job-dir gs://$BUCKET_NAME/friday_job_dir \\\n",
    "  --package-path ./friday_training \\\n",
    "  --module-name friday_training.train \\\n",
    "  --region us-east1 \\\n",
    "  --runtime-version=1.12 \\\n",
    "  --python-version=3.5 \\\n",
    "  --scale-tier BASIC \\\n",
    "  --stream-logs \\\n",
    "  -- \\\n",
    "  --bucket-name $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BlackFridayPredictor\"\n",
    "VERSION_NAME = \"friday_predictor\"\n",
    "#VERSION_NAME = \"friday_predictor_{}\".format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine models create $MODEL_NAME --regions us-east1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud ml-engine versions create $VERSION_NAME \\\n",
    "  --model=$MODEL_NAME \\\n",
    "  --framework=xgboost \\\n",
    "  --origin=gs://$BUCKET_NAME/ \\\n",
    "  --python-version=3.5 \\\n",
    "  --runtime-version=1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_10 = test_features.head(10)\n",
    "test_features_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FORMAT=\"text\" # JSON data format\n",
    "MODEL_NAME = \"BlackFridayPredictor\"\n",
    "VERSION_NAME = \"friday_predictor\"\n",
    "REGION='us-east1'\n",
    "JOB_NAME = \"friday_training_{}\".format(int(time.time()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_FILE=\"data.json\"\n",
    "\n",
    "!gcloud ai-platform predict --model $MODEL_NAME --version \\\n",
    "  $VERSION_NAME --json-instances $INPUT_FILE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $INPUT_FILE\n",
    "[1000004,1216,1,4,7,1,2,1,0,10,0,64902,83667,200699,20230,541656,1333]\n",
    "[1000009,1063,1,2,17,2,0,0,2,4,0,311554,57076,28791,37165,541656,371]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
